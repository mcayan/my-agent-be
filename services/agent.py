"""LangGraph æ™ºèƒ½ä½“å®ç°"""
import json
from typing import TypedDict, Annotated, Sequence, Literal
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage
from langchain_openai import ChatOpenAI
from langgraph.constants import START
from langgraph.graph import StateGraph, END

from core.config import settings
from services.image_tools import image_search_tool, image_generation_tool


# å®šä¹‰çŠ¶æ€
class AgentState(TypedDict):
    """æ™ºèƒ½ä½“çŠ¶æ€"""
    messages: Sequence[BaseMessage]
    user_input: str
    need_image_generation: bool
    search_query: str
    reference_image: dict
    generated_image: dict
    response: str


class ImageAgent:
    """å›¾ç‰‡ç”Ÿæˆæ™ºèƒ½ä½“"""
    
    def __init__(self):
        # åˆå§‹åŒ– LLM
        if not settings.OPENAI_API_KEY:
            raise ValueError("OPENAI_API_KEY is required in .env file")
        
        self.llm = ChatOpenAI(
            model="gpt-4o-mini",
            api_key=settings.OPENAI_API_KEY,
            base_url="https://api.openai-proxy.org/v1"
        )
        
        # æ„å»ºå›¾
        self.graph = self._build_graph()
    
    def _build_graph(self) -> StateGraph:
        """æ„å»º LangGraph å·¥ä½œæµ"""
        workflow = StateGraph(AgentState)
        
        # æ·»åŠ èŠ‚ç‚¹
        workflow.add_node("analyze_intent", self.analyze_intent)
        workflow.add_node("search_image", self.search_image)
        workflow.add_node("generate_image", self.generate_image)
        workflow.add_node("normal_chat", self.normal_chat)
        workflow.add_node("format_response", self.format_response)
        
        # è®¾ç½®å…¥å£
        workflow.add_edge(START,"analyze_intent")
        
        # æ·»åŠ æ¡ä»¶è¾¹
        workflow.add_conditional_edges(
            "analyze_intent",
            self.route_after_intent,
            {
                "search": "search_image",
                "chat": "normal_chat"
            }
        )
        
        workflow.add_edge("search_image", "generate_image")
        workflow.add_edge("generate_image", "format_response")
        workflow.add_edge("normal_chat", "format_response")
        workflow.add_edge("format_response", END)
        
        return workflow.compile()
    
    async def analyze_intent(self, state: AgentState) -> AgentState:
        """èŠ‚ç‚¹1: åˆ†æç”¨æˆ·æ„å›¾"""
        user_input = state["user_input"]
        
        # ä½¿ç”¨ LLM åˆ†ææ„å›¾
        prompt = f"""åˆ†æä»¥ä¸‹ç”¨æˆ·è¾“å…¥ï¼Œåˆ¤æ–­ç”¨æˆ·æ˜¯å¦éœ€è¦ç”Ÿæˆå›¾ç‰‡ã€‚

ç”¨æˆ·è¾“å…¥: {user_input}

å¦‚æœç”¨æˆ·æ˜ç¡®è¦æ±‚ç”Ÿæˆã€æœç´¢æˆ–è¦æŸä¸ªå›¾ç‰‡ï¼ˆæ¯”å¦‚"æˆ‘è¦ä¸€å¼ xxxçš„å›¾ç‰‡"ã€"ç»™æˆ‘ç”Ÿæˆxxxå›¾ç‰‡"ã€"å¸®æˆ‘æ‰¾xxxå›¾ç‰‡"ç­‰ï¼‰ï¼Œ
è¯·å›å¤ JSON æ ¼å¼: {{"need_image": true, "search_query": "æå–çš„æœç´¢å…³é”®è¯"}}

å¦‚æœæ˜¯æ™®é€šå¯¹è¯ï¼Œè¯·å›å¤: {{"need_image": false}}

åªè¿”å› JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚
"""
        
        try:
            response = await self.llm.ainvoke([HumanMessage(content=prompt)])
            response_text = response.content.strip()
            
            # è§£æ JSON å“åº”
            # æå– JSON éƒ¨åˆ†ï¼ˆé˜²æ­¢æœ‰å…¶ä»–æ–‡å­—ï¼‰
            if "```json" in response_text:
                json_start = response_text.find("```json") + 7
                json_end = response_text.find("```", json_start)
                response_text = response_text[json_start:json_end].strip()
            elif "```" in response_text:
                json_start = response_text.find("```") + 3
                json_end = response_text.find("```", json_start)
                response_text = response_text[json_start:json_end].strip()
            
            result = json.loads(response_text)
            
            state["need_image_generation"] = result.get("need_image", False)
            if state["need_image_generation"]:
                state["search_query"] = result.get("search_query", user_input)
            
        except Exception as e:
            print(f"Error analyzing intent: {e}")
            # é»˜è®¤ä½œä¸ºæ™®é€šå¯¹è¯å¤„ç†
            state["need_image_generation"] = False
        
        return state
    
    def route_after_intent(self, state: AgentState) -> Literal["search", "chat"]:
        """æ¡ä»¶è·¯ç”±: æ ¹æ®æ„å›¾å†³å®šä¸‹ä¸€æ­¥"""
        if state["need_image_generation"]:
            return "search"
        return "chat"
    
    async def search_image(self, state: AgentState) -> AgentState:
        """èŠ‚ç‚¹2: æœç´¢å‚è€ƒå›¾ç‰‡"""
        search_query = state["search_query"]
        
        print(f"Searching for: {search_query}")
        
        result = await image_search_tool.search_image(search_query)
        
        if result:
            state["reference_image"] = result
            print(f"Found reference image: {result['presigned_url']}")
        else:
            state["reference_image"] = {}
            print("No reference image found")
        
        return state
    
    async def generate_image(self, state: AgentState) -> AgentState:
        """èŠ‚ç‚¹3: ç”Ÿæˆæ–°å›¾ç‰‡"""
        user_input = state["user_input"]
        reference_image = state.get("reference_image", {})
        
        # ä½¿ç”¨ LLM ç”Ÿæˆæ›´å¥½çš„å›¾ç‰‡æç¤ºè¯
        prompt_generation = f"""åŸºäºç”¨æˆ·è¯·æ±‚ï¼Œç”Ÿæˆä¸€ä¸ªè¯¦ç»†çš„å›¾ç‰‡ç”Ÿæˆæç¤ºè¯ï¼ˆè‹±æ–‡ï¼‰ã€‚

ç”¨æˆ·è¯·æ±‚: {user_input}

è¯·ç”Ÿæˆä¸€ä¸ªè¯¦ç»†çš„è‹±æ–‡æç¤ºè¯ï¼Œæè¿°å›¾ç‰‡åº”è¯¥åŒ…å«çš„å†…å®¹ã€é£æ ¼ã€é¢œè‰²ç­‰ã€‚
åªè¿”å›æç¤ºè¯æœ¬èº«ï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚
"""
        
        try:
            response = await self.llm.ainvoke([HumanMessage(content=prompt_generation)])
            generation_prompt = response.content.strip()
        except Exception as e:
            print(f"Error generating prompt: {e}")
            generation_prompt = state["search_query"]
        
        print(f"Generation prompt: {generation_prompt}")
        
        # è°ƒç”¨å›¾ç‰‡ç”Ÿæˆ API
        reference_url = reference_image.get("presigned_url") if reference_image else None
        
        result = await image_generation_tool.generate_image_simple(
            prompt=generation_prompt,
            reference_image_url=reference_url
        )
        
        if result:
            state["generated_image"] = result
            print(f"Generated image: {result.get('presigned_url', 'N/A')}")
        else:
            state["generated_image"] = {}
            print("Failed to generate image")
        
        return state
    
    async def normal_chat(self, state: AgentState) -> AgentState:
        """èŠ‚ç‚¹4: æ™®é€šå¯¹è¯"""
        user_input = state["user_input"]
        messages = state.get("messages", [])
        
        # æ„å»ºå¯¹è¯å†å²
        chat_messages = [
            SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„ AI åŠ©æ‰‹ã€‚è¯·ç”¨ç®€æ´ã€è‡ªç„¶çš„æ–¹å¼å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚")
        ]
        chat_messages.extend(messages[-5:])  # ä¿ç•™æœ€è¿‘5æ¡æ¶ˆæ¯
        chat_messages.append(HumanMessage(content=user_input))
        
        try:
            response = await self.llm.ainvoke(chat_messages)
            state["response"] = response.content
        except Exception as e:
            print(f"Error in normal chat: {e}")
            state["response"] = "æŠ±æ­‰ï¼Œæˆ‘é‡åˆ°äº†ä¸€äº›é—®é¢˜ï¼Œè¯·ç¨åå†è¯•ã€‚"
        
        return state
    
    async def format_response(self, state: AgentState) -> AgentState:
        """èŠ‚ç‚¹5: æ ¼å¼åŒ–æœ€ç»ˆå“åº”"""
        if state["need_image_generation"]:
            reference_image = state.get("reference_image", {})
            generated_image = state.get("generated_image", {})
            
            response_parts = []
            
            if reference_image.get("success"):
                response_parts.append(
                    f"âœ… å·²æ‰¾åˆ°å‚è€ƒå›¾ç‰‡ï¼š{reference_image.get('title', 'æ ·ä¾‹å›¾ç‰‡')}"
                )
                response_parts.append(f"ğŸ“· å‚è€ƒå›¾ç‰‡é“¾æ¥ï¼š{reference_image['presigned_url']}")
            else:
                response_parts.append("âš ï¸ æœªæ‰¾åˆ°åˆé€‚çš„å‚è€ƒå›¾ç‰‡")
            
            if generated_image.get("success"):
                response_parts.append(
                    f"\nâœ¨ å·²æ ¹æ®æ‚¨çš„éœ€æ±‚ç”Ÿæˆæ–°å›¾ç‰‡ï¼"
                )
                response_parts.append(f"ğŸ¨ ç”Ÿæˆçš„å›¾ç‰‡é“¾æ¥ï¼š{generated_image['presigned_url']}")
                response_parts.append(f"ğŸ’¡ ç”Ÿæˆæç¤ºè¯ï¼š{generated_image.get('prompt', '')}")
            else:
                response_parts.append("\nâŒ å›¾ç‰‡ç”Ÿæˆå¤±è´¥ï¼Œè¯·ç¨åé‡è¯•")
            
            state["response"] = "\n".join(response_parts)
        
        return state
    
    async def run(self, user_input: str, conversation_history: list = None) -> dict:
        """
        è¿è¡Œæ™ºèƒ½ä½“
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            conversation_history: å¯¹è¯å†å²
            
        Returns:
            åŒ…å«å“åº”å’Œå›¾ç‰‡ä¿¡æ¯çš„å­—å…¸
        """
        # åˆå§‹åŒ–çŠ¶æ€
        initial_state = {
            "messages": conversation_history or [],
            "user_input": user_input,
            "need_image_generation": False,
            "search_query": "",
            "reference_image": {},
            "generated_image": {},
            "response": ""
        }
        
        # è¿è¡Œå›¾
        final_state = await self.graph.ainvoke(initial_state)
        
        return {
            "response": final_state["response"],
            "reference_image": final_state.get("reference_image"),
            "generated_image": final_state.get("generated_image"),
            "need_image_generation": final_state["need_image_generation"]
        }


# å…¨å±€æ™ºèƒ½ä½“å®ä¾‹
agent = ImageAgent()


